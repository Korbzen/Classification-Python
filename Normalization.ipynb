{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für das nächste Beispiel verwenden wir einen Datensatz bei dem Brustkrebs klassifiziert wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importieren eines Datensatzes\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "df_data = pd.DataFrame(data.data, columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.DataFrame(data['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine praktische Methode eines dataframes in pandas ist *describe*().\n",
    "\n",
    "Sie zeigt alle Features und ihre statistischen Werte an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilen des Datensatzes in test & training set\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df_data, df_target, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(Y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was bedeutet der Output nun für uns?\n",
    "\n",
    "Auf der linken Achse der Matrix stehen die realen Werte und auf der oberen die vorhergesagten.\n",
    "\n",
    "Unser Klassifizierer klassifiziert also:\n",
    "- 54 von 74 Leute ohne Brustkrebs richtig\n",
    "- 112 von 114 Leute mit Brustkrebs richtig\n",
    "    \n",
    "Das bedeutet, dass 20 Leute ohne Brustkrebs als krank(False Positive) und 2 Leute mit Brustkrebs als gesund(False Negative) klassifiziert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wie können wir unser Modell nun verbessern?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt mehrere Wege ein Machine Learning Model zu verbessern. Neben einem anderen Algorithmus gibt es vor allem im Preprocessing Wege es zu verbessern.\n",
    "- Feature Engineering: dabei wählt man bestimmte Features aus und versucht anhand von Metriken (z.B. Abhängigkeit der Variablen) das Ergebnis zu verbessern\n",
    "- Dimensionality Reduction: dabei wird der gesamte Feature Raum auf einen Unterraum abgebildet (Ein Beispiel hierfür ist PCA)\n",
    "- Normalisieren: Alle Variablen auf den gleichen Werteraum normalisieren\n",
    "- Hyperparameter optimieren\n",
    "\n",
    "Wir werden uns im nächsten Abschnitt mit den letzteren drei Möglichkeiten beschäftigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zunächst importieren wir wieder die notwendigen Pakete\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst normalisieren wir alle Werte in unserem Datensatz auf einen Abschnitt zwischen 0 & 1.\n",
    "\n",
    "Dadurch kann der Algorithmus besser konvergieren, da der Fehler nicht so schnell ansteigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = MinMaxScaler().fit_transform(df_data)\n",
    "# the output from scikit-learn methods are numpy arrays so we need to transform it into a dataframe\n",
    "df_data_scaled = pd.DataFrame(data_scaled)\n",
    "df_data_scaled.columns = df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun sind alle unsere Werte auf den Bereich 0...1 normalisiert.\n",
    "Versuchen wir jetzt noch einmal das selbe Modell zu trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = SVC(gamma='scale')\n",
    "# Split the datasets again\n",
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(df_data_scaled, df_target, test_size=0.33)\n",
    "clf2.fit(X_train2, Y_train2)\n",
    "prediction2 = clf2.predict(X_test2)\n",
    "\n",
    "confusion_matrix(Y_test2, prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir sehen ist unsere Anzahl an False Positives sehr stark gesunken.\n",
    "\n",
    "Das liegt daran, dass unsere Werte nun alle die gleiche Wertigkeit haben.\n",
    "\n",
    "Bildlich bedeutet das, dass eine Messung die zwischen 0...1 liegt nun die gleiche Gewichtung hat, wie -100...100.\n",
    "\n",
    "## Hyperparameter optimieren\n",
    "Machine Learning Algorithmen haben, wie wir schon mit der Lernrate beim Perceptron gesehen haben Hyperparameter.\n",
    "\n",
    "Diese können wir nutzen um unseren Algorithmus zu verbessern.\n",
    "\n",
    "Die Attribute der SVC Klasse findet man unter: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ein Beispiel\n",
    "\n",
    "clf3 = SVC(gamma='scale', degree=4)\n",
    "# Split the datasets again\n",
    "clf3.fit(X_train2, Y_train2)\n",
    "prediction3 = clf3.predict(X_test2)\n",
    "\n",
    "confusion_matrix(Y_test2, prediction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#\n",
    "#\n",
    "# Versuche es hier einmal selbst\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dimensionsreduktion\n",
    "Nun nutzen wir Principal Component Analysis. Der Algorithmus sucht eine Funktion die Varianz des Datensatzes mit der so wenig Variablen wie möglich abzubilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "data_reduced = pca.fit_transform(df_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_reduced = pd.DataFrame(data_reduced)\n",
    "df_data_reduced.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "var_exp =  pca.explained_variance_ratio_\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "plt.bar(range(1,11), var_exp, alpha=0.5, align='center',\n",
    "        label='individual explained variance')\n",
    "plt.step(range(1,11), cum_var_exp, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir sehen können beschreiben die ersten 5 Dimensionen u\"ber 90% der Varianz im Datensatz (also auch des Informationsgehalts)\n",
    "Wir können also die PCA auf lediglich 5 Komponenten reduzieren lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "data_reduced = pca.fit_transform(df_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_reduced = SVC(gamma='scale')\n",
    "# Split the datasets again\n",
    "X_train3, X_test3, Y_train3, Y_test3 = train_test_split(df_data_reduced, df_target, test_size=0.33)\n",
    "clf_reduced.fit(X_train3, Y_train3)\n",
    "prediction_red = clf_reduced.predict(X_test3)\n",
    "\n",
    "confusion_matrix(Y_test3, prediction_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn ihr es noch mit anderen Klassifizierern versuchen wollt, findet ihr auf dieser Seite ein Skript mit dem Vergleich mehrerer Klassifizierer.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
